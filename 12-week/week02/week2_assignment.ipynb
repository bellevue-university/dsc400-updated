{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "df0fc668",
      "metadata": {
        "id": "df0fc668"
      },
      "source": [
        "# Part1: Data Collection and Wrangling\n",
        "\n",
        "## Introduction to Data Collection\n",
        "- Data collection is the systematic approach to gathering and measuring information from varied sources to get a complete and accurate picture of an area of interest.\n",
        "- In the realm of big data, it's not just about the quantity of data but also about the diversity of sources and types of data we collect, ranging from traditional databases to the internet of things (IoT).\n",
        "\n",
        "## Web Scraping\n",
        "- Web scraping is a valuable tool for data collection that involves extracting data from websites.\n",
        "- It's a practical approach to convert the data found on web pages into a structured format that we can analyze and use in various applications.\n",
        "\n",
        "## Key Python Libraries\n",
        "- `requests`: Allows you to send HTTP requests easily, access the content and get the data from the response.\n",
        "- `BeautifulSoup` from `bs4`: A Python package for parsing HTML and XML documents. It creates parse trees that can be used to extract data from HTML, which is very useful for web scraping.\n",
        "\n",
        "## Data Wrangling\n",
        "- Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from a raw data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes, such as analytics.\n",
        "- A major aspect of data wrangling is data cleaning which can involve a wide array of tasks such as removing duplicates, correcting errors, and dealing with missing values.\n",
        "\n",
        "## Pandas for Data Wrangling\n",
        "- `pandas` is a cornerstone library for data analysis in Python, providing fast, flexible, and expressive data structures designed to make working with \"relational\" or \"labeled\" data both easy and intuitive.\n",
        "- It offers data structures like DataFrames and Series, along with a comprehensive set of operations for data manipulation, cleaning, and preparation.\n",
        "\n",
        "## Ethical Considerations\n",
        "- Respect the `robots.txt` file and terms of service of the website.\n",
        "- Avoid sending too many requests to the website in a short period, which might overload the server.\n",
        "\n",
        "## Legal Considerations\n",
        "- Ensure that your web scraping activities are legal and do not infringe on copyright laws or privacy regulations.\n",
        "- Always be transparent about your data collection methods and purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70cd7158",
      "metadata": {
        "id": "70cd7158"
      },
      "source": [
        "# Part 2: Follow Me - Web Scraping and Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "79a4aacb",
      "metadata": {
        "id": "79a4aacb"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "389ca8d0",
      "metadata": {
        "id": "389ca8d0"
      },
      "source": [
        "Web Scraping Example: Retrieving Quotes and Authors\n",
        "We will begin with a simple web scraping example using the 'requests' and 'BeautifulSoup' libraries to retrieve quotes and their authors from a demo website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a4d83bc3",
      "metadata": {
        "id": "a4d83bc3"
      },
      "outputs": [],
      "source": [
        "# Sending a GET request to the website\n",
        "url = 'http://quotes.toscrape.com/'\n",
        "response = requests.get(url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensuring we received a successful response\n",
        "if response.ok:\n",
        "    print(\"Response is successful!\")\n",
        "else:\n",
        "    print(\"Failed to retrieve data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj0jKTzTc8n7",
        "outputId": "a30d6e99-2167-4ab3-c175-8df28e999f9a"
      },
      "id": "Qj0jKTzTc8n7",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response is successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsing the page using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "print(\"Page parsed with BeautifulSoup.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uInD_qO_c9EW",
        "outputId": "73cdbc3f-6b1b-4375-bd08-ebb61984229c"
      },
      "id": "uInD_qO_c9EW",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page parsed with BeautifulSoup.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding all the span tags that contain quotes\n",
        "quotes = soup.find_all('span', class_='text')\n",
        "# Finding all the small tags that contain authors\n",
        "authors = soup.find_all('small', class_='author')\n",
        "\n",
        "# Extracting the text from the quotes and authors\n",
        "quotes_text = [quote.get_text() for quote in quotes]\n",
        "authors_names = [author.get_text() for author in authors]\n",
        "print(\"Extracted quotes and authors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs0mVgWlc6NL",
        "outputId": "d519e9ce-e86a-41ba-b704-344223fe081c"
      },
      "id": "Hs0mVgWlc6NL",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted quotes and authors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a DataFrame from the scraped data\n",
        "quotes_df = pd.DataFrame({\n",
        "    'Quote': quotes_text,\n",
        "    'Author': authors_names\n",
        "})\n",
        "\n",
        "# Displaying the first few entries of the DataFrame\n",
        "print(\"Scraped Data:\")\n",
        "print(quotes_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eubmAXdCdAUl",
        "outputId": "1fba7c97-7b0c-4f3c-d684-9a761b1a9130"
      },
      "id": "eubmAXdCdAUl",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped Data:\n",
            "                                               Quote           Author\n",
            "0  “The world as we have created it is a process ...  Albert Einstein\n",
            "1  “It is our choices, Harry, that show what we t...     J.K. Rowling\n",
            "2  “There are only two ways to live your life. On...  Albert Einstein\n",
            "3  “The person, be it gentleman or lady, who has ...      Jane Austen\n",
            "4  “Imperfection is beauty, madness is genius and...   Marilyn Monroe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before dropping duplicates\n",
        "print(\"\\nNumber of entries before removing duplicates:\", quotes_df.shape[0])\n",
        "\n",
        "# Removing duplicate rows\n",
        "quotes_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# After dropping duplicates\n",
        "print(\"Number of entries after removing duplicates:\", quotes_df.shape[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS9a5wTVdBkV",
        "outputId": "a19b033a-02d1-4069-d9c8-af2e0c1f910d"
      },
      "id": "qS9a5wTVdBkV",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of entries before removing duplicates: 10\n",
            "Number of entries after removing duplicates: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "470751b8",
      "metadata": {
        "id": "470751b8"
      },
      "source": [
        "# Part 3: Your Turn - Advanced Web Scraping and Data Wrangling\n",
        "\n",
        "This part of the course will engage you in a real-world web scraping and data wrangling project. You'll apply the techniques you've learned to extract data from the web and then clean and prepare it for analysis. This will involve handling various data complexities such as missing values and format inconsistencies.\n",
        "\n",
        "## Web Scraping:\n",
        "- Use the `requests` and `BeautifulSoup` libraries to perform web scraping.\n",
        "- Identify a website that allows scraping and collect data, ensuring you comply with any terms of service.\n",
        "\n",
        "## Data Cleaning:\n",
        "- Data may come with missing values or inconsistencies. Determine the best strategies for handling these issues, whether it's filling in missing values (imputation) or removing affected rows.\n",
        "- Clean your data to ensure accuracy in your analysis, which might involve addressing outliers or errors.\n",
        "\n",
        "## Feature Engineering:\n",
        "- Consider ways to enrich your data, such as creating new columns that summarize or combine existing information in useful ways.\n",
        "\n",
        "## Advanced Aggregations:\n",
        "- Use pandas functionality such as `.groupby()` to organize your data by relevant categories and compute aggregate statistics.\n",
        "\n",
        "## Visualization:\n",
        "- Translate your data into visual form with libraries like Matplotlib or Seaborn. Choose plots that effectively communicate the patterns you've discovered.\n",
        "\n",
        "## Instructions:\n",
        "1. Scrape data from your chosen website using `requests` and `BeautifulSoup`.\n",
        "2. Load the scraped data into a pandas DataFrame.\n",
        "3. Perform necessary data cleaning and feature engineering.\n",
        "4. Aggregate and analyze the data to find trends and insights.\n",
        "5. Visualize your findings with appropriate graphs.\n",
        "6. Compile your steps and insights into the Jupyter notebook and submit it as your completed assignment."
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}