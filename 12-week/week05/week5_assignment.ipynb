{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Big Data Processing with Apache Spark\n",
        "\n",
        "## Introduction to Apache Spark\n",
        "- Apache Spark is a powerful open-source distributed computing system designed for large-scale data processing.\n",
        "- It offers a unified analytics engine that supports a wide range of workloads, including batch processing, interactive queries, streaming analytics, and machine learning.\n",
        "\n",
        "## Importance of Apache Spark\n",
        "- **Speed:** Spark processes data in-memory, making it much faster than traditional disk-based systems like Hadoop MapReduce.\n",
        "- **Ease of Use:** Provides high-level APIs in Python, Java, Scala, and R, making it accessible to a wide range of developers and data scientists.\n",
        "- **Flexibility:** Spark can integrate with various data sources and storage systems, including HDFS, S3, Cassandra, and HBase, enabling seamless data processing across different platforms.\n",
        "\n",
        "## Key Components of Apache Spark\n",
        "- **Resilient Distributed Datasets (RDDs):** The fundamental data structure in Spark, RDDs are fault-tolerant collections of data that can be processed in parallel across a cluster.\n",
        "- **DataFrames:** Spark's DataFrame API provides a higher-level abstraction for working with structured data, similar to a relational database or a spreadsheet.\n",
        "- **Spark SQL:** Allows you to execute SQL queries against Spark DataFrames, enabling seamless integration of SQL-based operations with Spark's distributed computing capabilities.\n",
        "- **Machine Learning Library (MLlib):** Provides scalable machine learning algorithms and utilities for data preprocessing, feature engineering, and model training.\n",
        "- **Streaming:** Spark Streaming allows for real-time processing of data streams, enabling applications such as real-time analytics and event detection.\n",
        "\n",
        "## Real-World Application: Fraud Detection\n",
        "- Apache Spark is widely used in fraud detection systems, where large volumes of transaction data need to be analyzed in real-time to identify fraudulent activities.\n",
        "- Spark's speed and scalability make it ideal for processing streaming data and performing complex analytics to detect patterns indicative of fraud.\n",
        "\n",
        "## Getting Started with Apache Spark\n",
        "- To begin using Apache Spark, you need to install Spark on a cluster or a single machine. You can download the Spark distribution from the official website and follow the installation instructions provided in the documentation.\n",
        "- Once Spark is installed, you can start writing Spark applications using your preferred programming language and Spark APIs.\n",
        "- Spark provides extensive documentation and tutorials to help you get started with building big data applications using Spark.\n",
        "\n"
      ],
      "metadata": {
        "id": "h_iui_GrZpmV"
      },
      "id": "h_iui_GrZpmV"
    },
    {
      "cell_type": "markdown",
      "id": "e9f65ab5",
      "metadata": {
        "id": "e9f65ab5"
      },
      "source": [
        "# Part 2: Follow Me - Comprehensive Data Processing with PySpark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyspark"
      ],
      "metadata": {
        "id": "vc2HJgybaMxq"
      },
      "id": "vc2HJgybaMxq",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "52fd272e",
      "metadata": {
        "id": "52fd272e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, expr\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.streaming import StreamingContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "001872be",
      "metadata": {
        "id": "001872be"
      },
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('Spark Fundamentals').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1f56b7bf",
      "metadata": {
        "id": "1f56b7bf"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = spark.read.csv('retail_sales.csv', inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "609fe4f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "609fe4f0",
        "outputId": "cd44c481-bb25-44ce-9bcd-7a98e1957e86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying the first 5 elements of the modified RDD:\n",
            "[(1, 160.08), (2, 48.58), (3, 651.49), (4, 62.81), (5, 368.36)]\n"
          ]
        }
      ],
      "source": [
        "# Exploring RDDs\n",
        "# Convert DataFrame to RDD and use a lambda function to transform the data\n",
        "rdd = df.rdd\n",
        "modified_rdd = rdd.map(lambda x: (x[0], x[2] * x[3]))  # Multiply Quantity by Price\n",
        "print(\"Displaying the first 5 elements of the modified RDD:\")\n",
        "print(modified_rdd.take(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4751540e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4751540e",
        "outputId": "23b46a9b-c1ca-4b96-b2de-7fb2d6eb95eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------+--------+-----+-------------------+-------+----------+\n",
            "|TransactionID|ProductID|Quantity|Price|          Timestamp|StoreID|TotalValue|\n",
            "+-------------+---------+--------+-----+-------------------+-------+----------+\n",
            "|            1|     3732|       4|40.02|2021-02-22 15:57:50|     17|    160.08|\n",
            "|            2|     3607|       2|24.29|2021-12-19 06:49:23|     34|     48.58|\n",
            "|            3|     2653|       7|93.07|2021-10-11 04:06:41|     43|    651.49|\n",
            "|            4|     4264|       1|62.81|2021-06-16 12:11:54|     35|     62.81|\n",
            "|            5|     1835|       4|92.09|2021-08-07 04:16:30|      1|    368.36|\n",
            "+-------------+---------+--------+-----+-------------------+-------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Exploring DataFrames\n",
        "# Using DataFrame API to add a new column and display the transformed DataFrame\n",
        "df = df.withColumn(\"TotalValue\", col(\"Quantity\") * col(\"Price\"))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d4336df9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4336df9",
        "outputId": "c2ec5a54-c357-498a-e1a1-85821302451c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+\n",
            "|StoreID|        TotalSales|\n",
            "+-------+------------------+\n",
            "|     31|49088.509999999995|\n",
            "|     34|46794.260000000024|\n",
            "|     28|53972.140000000036|\n",
            "|     27| 48694.81999999999|\n",
            "|     26|          54925.67|\n",
            "|     44| 69458.61000000003|\n",
            "|     12| 46701.61999999999|\n",
            "|     22| 51343.63999999999|\n",
            "|     47|          54471.49|\n",
            "|      1|41863.930000000044|\n",
            "|     13| 47859.61000000001|\n",
            "|     16| 49360.24999999999|\n",
            "|      6|           53295.4|\n",
            "|      3|          45748.31|\n",
            "|     20|           48713.9|\n",
            "|     40|51067.779999999984|\n",
            "|     48|           49294.3|\n",
            "|      5| 55946.81999999997|\n",
            "|     19| 55494.18000000001|\n",
            "|     41| 41796.73999999998|\n",
            "+-------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Exploring Spark SQL\n",
        "# Register DataFrame as a temporary view and execute SQL queries\n",
        "df.createOrReplaceTempView(\"sales\")\n",
        "total_sales = spark.sql(\"SELECT StoreID, SUM(TotalValue) as TotalSales FROM sales GROUP BY StoreID\")\n",
        "total_sales.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Your Turn - Advanced Data Processing Task\n",
        "\n",
        "This part of the course challenges you to engage in a comprehensive data processing project using Apache Spark. You will apply techniques learned in Part 2 to a new dataset, addressing real-world data complexities such as missing values, anomalies, and the need for advanced aggregations.\n",
        "\n",
        "## Data Exploration:\n",
        "- Investigate the 'Retail Sales' to discover patterns, insights, and anomalies. This step is crucial for understanding the data's structure and content before proceeding with transformations.\n",
        "\n",
        "## Data Cleaning:\n",
        "- Address any missing values or format inconsistencies in your dataset. This is essential to ensure the accuracy of your data processing and analysis.\n",
        "\n",
        "## Feature Engineering:\n",
        "- Enhance your dataset by creating new features that can provide more depth to your analysis. For example, you might calculate the duration of customer sessions or categorize user activity based on engagement levels.\n",
        "\n",
        "## Spark SQL:\n",
        "- Use Spark SQL to perform sophisticated data aggregations that reveal underlying trends. For instance, you might want to analyze the frequency of specific event types per customer or per session.\n",
        "\n",
        "\n",
        "## Advanced Aggregations:\n",
        "- Utilize Sparkâ€™s capabilities to perform complex aggregations like calculating the average session time per browser type or the total interactions per day.\n",
        "\n",
        "## Visualization and Reporting:\n",
        "- Create meaningful static and interactive visualizations to represent your findings using Spark or external libraries like `matplotlib` or `plotly`.\n",
        "- Craft visual representations of your analytical findings to showcase patterns and insights effectively.\n",
        "\n",
        "## Instructions:\n",
        "1. Load the 'Customer Interactions Dataset' into Spark and perform initial explorations to understand its structure.\n",
        "2. Conduct necessary data cleaning and feature engineering to prepare your data for deeper analysis.\n",
        "3. Use Spark SQL to perform detailed analyses.\n",
        "5. Compile your steps and insights into the Jupyter notebook and submit it as your completed assignment."
      ],
      "metadata": {
        "id": "Hb7LXykVaGYF"
      },
      "id": "Hb7LXykVaGYF"
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}