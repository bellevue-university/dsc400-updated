{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8ae03a5b",
      "metadata": {
        "id": "8ae03a5b"
      },
      "source": [
        "## Part 1: Lecture - Introduction to Large Language Models (LLMs) for Big Data Analytics\n",
        "\n",
        "### Introduction to Large Language Models (LLMs) for Big Data Analytics:\n",
        "- Large Language Models (LLMs) are advanced AI models designed to understand and generate human language.\n",
        "- Examples include GPT-2, GPT-3, and BERT.\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "**Understanding LLMs:**\n",
        "- **Large Language Models**:\n",
        "  - LLMs have billions of parameters.\n",
        "  - Capable of understanding context and generating coherent text.\n",
        "  - Perform various NLP tasks such as translation, summarization, and question answering.\n",
        "- **Pre-training and Fine-tuning**:\n",
        "  - LLMs are pre-trained on large corpora to learn language patterns.\n",
        "  - Fine-tuning on specific datasets enhances performance on particular tasks.\n",
        "\n",
        "**Applications of LLMs:**\n",
        "- **Text Generation**:\n",
        "  - Generates human-like text.\n",
        "  - Useful for writing assistants, content creation, and storytelling.\n",
        "- **Chatbots and Virtual Assistants**:\n",
        "  - Powers intelligent chatbots and virtual assistants.\n",
        "  - Understands and responds to user queries naturally.\n",
        "- **Sentiment Analysis**:\n",
        "  - Analyzes sentiment in text.\n",
        "  - Helps businesses understand customer opinions and emotions.\n",
        "- **Translation and Summarization**:\n",
        "  - Translates text between languages.\n",
        "  - Summarizes long documents into concise versions.\n",
        "\n",
        "### Python Libraries:\n",
        "- **Hugging Face Transformers**:\n",
        "  - A powerful library for working with transformer models, including LLMs like GPT-2 and GPT-3.\n",
        "- **PyTorch**:\n",
        "  - A deep learning library providing flexibility and speed in building and training models.\n",
        "\n",
        "### Challenges and Considerations:\n",
        "- **Computational Resources**:\n",
        "  - Training and fine-tuning LLMs require significant computational power and memory.\n",
        "- **Data Privacy**:\n",
        "  - Ensuring data used for training and inference respects privacy and confidentiality.\n",
        "- **Ethical Use**:\n",
        "  - Addressing the potential misuse of LLMs in generating harmful or misleading content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09ae172c",
      "metadata": {
        "id": "09ae172c"
      },
      "source": [
        "# Part 2: Code Walkthrough - Fine-tuning a Large Language Model (LLM)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Packages\n",
        "# !pip install transformers>=4.11.3\n",
        "# !pip install accelerate>=0.21.0\n",
        "# !pip install \"transformers[torch]>=4.11.3\"\n",
        "\n",
        "## Restart Kernel"
      ],
      "metadata": {
        "id": "5kOLKqJHrFQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4389c982-676a-4d50-eb4e-ac6a03105628"
      },
      "id": "5kOLKqJHrFQS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch]>=4.11.3 in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]>=4.11.3) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]>=4.11.3) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]>=4.11.3) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]>=4.11.3) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]>=4.11.3) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]>=4.11.3) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]>=4.11.3) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]>=4.11.3) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]>=4.11.3) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]>=4.11.3) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]>=4.11.3) (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]>=4.11.3) (0.29.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]>=4.11.3) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]>=4.11.3) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]>=4.11.3) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]>=4.11.3) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]>=4.11.3) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]>=4.11.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]>=4.11.3) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]>=4.11.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]>=4.11.3) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]>=4.11.3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]>=4.11.3) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21fdbcf7",
      "metadata": {
        "id": "21fdbcf7"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b710862d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b710862d",
        "outputId": "5791ff42-1749-4534-f515-8019788acf57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = 'gpt2-medium'  # Change to the desired pre-trained model size\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87a2534",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b87a2534",
        "outputId": "d5c74175-1589-41a1-95ec-e7dc7bef3df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess your dataset\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"train.txt\",  # Path to your training dataset file\n",
        "    block_size=128  # Adjust block size as needed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d53cfb92",
      "metadata": {
        "id": "d53cfb92"
      },
      "outputs": [],
      "source": [
        "# Define training arguments and trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"llm-fine-tuned\",  # Directory to save the fine-tuned model and logs\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    per_device_train_batch_size=8,  # Batch size per GPU/CPU during training\n",
        "    save_steps=1000,  # Save model checkpoint every specified number of steps\n",
        "    save_total_limit=2,  # Limit the total number of saved checkpoints\n",
        "    prediction_loss_only=True,  # Only compute the prediction loss\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9729cfd",
      "metadata": {
        "id": "c9729cfd"
      },
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False\n",
        "    ),\n",
        "    train_dataset=train_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb6743e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "fb6743e5",
        "outputId": "0ab8f81b-a119-47e6-8df6-47c4eed0f984"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=2.2804082234700522, metrics={'train_runtime': 50.5568, 'train_samples_per_second': 0.059, 'train_steps_per_second': 0.059, 'total_flos': 696525520896.0, 'train_loss': 2.2804082234700522, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Fine-tune the pre-trained model on your dataset\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd9c250",
      "metadata": {
        "id": "ddd9c250"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"llm-fine-tuned\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fine-tuned model for testing\n",
        "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"llm-fine-tuned\")\n",
        "\n",
        "# Define a prompt or input text for generation\n",
        "prompt = \"In recent years, artificial intelligence has revolutionized\"\n",
        "\n",
        "# Tokenize the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text based on the prompt\n",
        "output = fine_tuned_model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=0.7, do_sample=True)\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtCmS5pmtiIr",
        "outputId": "b426dc73-5ba6-414a-f1a1-14baa39e615c"
      },
      "id": "FtCmS5pmtiIr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "In recent years, artificial intelligence has revolutionized the way we think about human relationships, the way we think about business, and the way we think about our jobs.\n",
            "\n",
            "As machine learning continues to improve, we'll see AI-driven businesses and services create new ways of doing business.\n",
            "\n",
            "As AI-driven businesses and services create new ways of doing business, we'll see companies such as Uber and Airbnb gain new customers.\n",
            "\n",
            "As AI-driven companies and services gain new customers,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Your Turn - Building and Tuning Your Own Large Language Model\n",
        "\n",
        "This part of the course empowers you to build and fine-tune your own Large Language Model using the Hugging Face Transformers library. You'll choose a specific dataset tailored to an application of interest, such as chatbot responses or literary analysis, and apply deep learning techniques to optimize your model for that particular task.\n",
        "\n",
        "## Dataset Selection:\n",
        "- Identify and select a dataset that aligns with the text generation application you wish to focus on. Ensure the dataset is suitable for NLP tasks and preprocess it accordingly.\n",
        "\n",
        "## Model Preparation:\n",
        "- Choose a pre-trained model from the Hugging Face Model Hub that best fits your chosen application. Initialize this model along with its tokenizer.\n",
        "\n",
        "## Model Fine-Tuning:\n",
        "- Customize the training parameters such as learning rate, batch size, and number of epochs to optimize your model's performance for the specific type of text you're working with.\n",
        "\n",
        "## Practical Applications:\n",
        "- Develop practical text generation tasks that utilize your fine-tuned model. This could involve generating creative text, automating customer service responses, or providing analytical summaries.\n",
        "\n",
        "## Model Evaluation:\n",
        "- Evaluate your model using metrics suitable for text generation, such as perplexity or BLEU score, to understand its effectiveness and areas for improvement.\n",
        "\n",
        "## Iterative Improvement:\n",
        "- Refine your model based on performance feedback. Experiment with different configurations and training techniques to enhance its accuracy and response quality.\n",
        "\n",
        "## Instructions:\n",
        "1. Select and preprocess a dataset appropriate for your application.\n",
        "2. Configure and load a pre-trained model suitable for your text generation task.\n",
        "3. Fine-tune the model on your dataset with customized training parameters.\n",
        "4. Implement and test the model across various text generation tasks to demonstrate its capabilities.\n",
        "5. Evaluate the performance of your model, making iterative improvements based on your findings.\n",
        "6. Document all your processes, from model selection to final evaluations, in a Jupyter notebook and prepare a detailed report summarizing your methodology, results, and insights.\n",
        "7. Compile your steps and insights into the Jupyter notebook and submit it as your completed assignment.\n"
      ],
      "metadata": {
        "id": "tRo-AoM5uJf-"
      },
      "id": "tRo-AoM5uJf-"
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}