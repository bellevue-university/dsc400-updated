{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Introduction to Machine Learning in Big Data\n",
        "\n",
        "## Introduction to Machine Learning in Big Data:\n",
        "Machine learning involves teaching computers to learn from data, identify patterns,\n",
        "and make decisions with minimal human intervention. With big data, these processes\n",
        "involve larger datasets and require more advanced techniques to manage and analyze data effectively.\n",
        "\n",
        "### Key ML Concepts:\n",
        "- **Supervised Learning:** In supervised learning, the algorithm learns from labeled data, i.e., data that is already tagged with the correct answer. It learns to predict the output from the input data.\n",
        "- **Unsupervised Learning:** Unsupervised learning allows the algorithm to act on its own to discover patterns in the data. It explores the data and identifies hidden structures without any prior information about labels.\n",
        "- **Model Evaluation:** Model evaluation is essential to assess the performance of a machine learning model. Metrics such as accuracy, precision, recall, and F1 score are commonly used to measure how well the model performs on unseen data.\n",
        "\n",
        "### Python Libraries for ML:\n",
        "- `scikit-learn`: Scikit-learn is a comprehensive library for machine learning in Python. It provides simple and efficient tools for data mining and data analysis, built on NumPy, SciPy, and Matplotlib.\n",
        "- `pyspark MLlib`: MLlib is Apache Spark's scalable machine learning library. It provides a wide range of algorithms and utilities for large-scale machine learning tasks, including classification, regression, clustering, and collaborative filtering.\n",
        "\n",
        "### Challenges of ML with Big Data:\n",
        "- **Scalability:** Dealing with large volumes of data requires algorithms and techniques that can scale efficiently to handle the increased computational load.\n",
        "- **Data Quality:** Big data often comes with challenges related to data quality, such as missing values, inconsistencies, and noise. Managing and cleaning such data is crucial for building accurate machine learning models.\n",
        "- **Computational Complexity:** Processing and analyzing large datasets can be computationally intensive and time-consuming. Efficient algorithms and distributed computing frameworks like Apache Spark are necessary to tackle these challenges effectively.\n"
      ],
      "metadata": {
        "id": "XZ7FwKtufcPq"
      },
      "id": "XZ7FwKtufcPq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Follow Me - Building a Machine Learning Model with scikit-learn\n"
      ],
      "metadata": {
        "id": "4NlBdkpif2cV"
      },
      "id": "4NlBdkpif2cV"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d84fc13f",
      "metadata": {
        "id": "d84fc13f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7217344e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7217344e",
        "outputId": "85e91745-a4cd-4c1a-d0ec-89020279f800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Loading Data\n",
        "# Load the healthcare dataset from a CSV file into a pandas DataFrame.\n",
        "healthcare_df = pd.read_csv('healthcare.csv')\n",
        "print(\"Data loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e9852479",
      "metadata": {
        "id": "e9852479"
      },
      "outputs": [],
      "source": [
        "# Step 2: Data Preprocessing\n",
        "# Check for missing values and handle them as necessary (omitted here for brevity).\n",
        "# Assuming the target variable 'Disease_Presence' is categorical and needs encoding if it's in string format.\n",
        "le = LabelEncoder()\n",
        "healthcare_df['Disease_Presence'] = le.fit_transform(healthcare_df['Disease_Presence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f3fa7004",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3fa7004",
        "outputId": "ba3d7fd5-637b-4ccf-9db7-42c58d5510af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing completed.\n"
          ]
        }
      ],
      "source": [
        "# Separate the features and the target variable.\n",
        "X = healthcare_df.drop('Disease_Presence', axis=1)\n",
        "y = healthcare_df['Disease_Presence']\n",
        "print(\"Data preprocessing completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2d3da009",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d3da009",
        "outputId": "bd175997-2f2e-4cd7-8f49-2cbcddaf21db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split into training and testing sets.\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Splitting the Dataset\n",
        "# Split the dataset into training and testing sets to evaluate the model's performance.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print(\"Dataset split into training and testing sets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c0ccfe0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0ccfe0a",
        "outputId": "66c3be7f-d534-4861-b613-86cf56d12449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete.\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Model Building\n",
        "# Initialize and train a Random Forest Classifier.\n",
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "97e5eea3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97e5eea3",
        "outputId": "bf06a5b3-e3fa-4ffa-81ae-8f3b661702a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model evaluation results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.46      0.47      1489\n",
            "           1       0.50      0.52      0.51      1511\n",
            "\n",
            "    accuracy                           0.49      3000\n",
            "   macro avg       0.49      0.49      0.49      3000\n",
            "weighted avg       0.49      0.49      0.49      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Model Evaluation\n",
        "# Use the test set to make predictions and evaluate the model.\n",
        "predictions = clf.predict(X_test)\n",
        "print(\"Model evaluation results:\")\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8Lh20uqgZzo",
        "outputId": "4d3717df-02eb-4c81-c680-168ccb1e1888"
      },
      "id": "N8Lh20uqgZzo",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=a551161fad97ebf89d87fbe29ca1b5fb0688fb29d663e025cdb96b07f82b35ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Now with Spark MLIB\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"HealthcareMLlib\").getOrCreate()\n",
        "\n",
        "# Load the healthcare dataset with Spark\n",
        "healthcare_spark_df = spark.read.csv(\"healthcare.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "# Handle missing values (e.g., fill or drop - here we'll drop for simplicity)\n",
        "healthcare_spark_df = healthcare_spark_df.na.drop()\n",
        "\n",
        "# Step 2: Encode categorical target variable if necessary\n",
        "indexer = StringIndexer(inputCol=\"Disease_Presence\", outputCol=\"label\")\n",
        "healthcare_spark_df = indexer.fit(healthcare_spark_df).transform(healthcare_spark_df)\n",
        "\n",
        "# Step 3: Assemble features into a single vector\n",
        "feature_cols = healthcare_spark_df.columns\n",
        "feature_cols.remove(\"Disease_Presence\")\n",
        "feature_cols.remove(\"label\")\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "healthcare_spark_df = assembler.transform(healthcare_spark_df)\n",
        "\n",
        "# Step 4: Split the data into training and test sets\n",
        "train_df, test_df = healthcare_spark_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Step 5: Apply the Random Forest Classifier\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
        "rf_model = rf.fit(train_df)\n",
        "\n",
        "# Step 6: Make predictions on the test set\n",
        "predictions = rf_model.transform(test_df)\n",
        "\n",
        "# Step 7: Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRC5NNvkgXUG",
        "outputId": "f27efaf8-f88c-4c27-e388-92d70bca93d5"
      },
      "id": "XRC5NNvkgXUG",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.5075481520041645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Your Turn - Advanced Data Processing and Machine Learning with Spark MLlib\n",
        "\n",
        "This part of the course challenges you to engage in a comprehensive data processing and machine learning project using Apache Spark. You will apply techniques learned in Part 2 to a new dataset, addressing real-world data complexities such as missing values, anomalies, and the need for advanced aggregations. Additionally, you will implement a machine learning model using Spark MLlib.\n",
        "\n",
        "## Data Exploration:\n",
        "- Investigate the 'Telecom Customer Churn' dataset to discover patterns, insights, and anomalies. This step is crucial for understanding the data's structure and content before proceeding with transformations.\n",
        "\n",
        "## Data Cleaning:\n",
        "- Address any missing values or format inconsistencies in your dataset. This is essential to ensure the accuracy of your data processing and analysis.\n",
        "\n",
        "## Feature Engineering:\n",
        "- Enhance your dataset by creating new features that can provide more depth to your analysis. For example, you might calculate the duration of customer sessions or categorize user activity based on engagement levels.\n",
        "\n",
        "## Spark SQL:\n",
        "- Use Spark SQL to perform sophisticated data aggregations that reveal underlying trends. For instance, you might want to analyze the frequency of specific event types per customer or per session.\n",
        "\n",
        "## Advanced Aggregations:\n",
        "- Utilize Spark’s capabilities to perform complex aggregations like calculating the average session time per browser type or the total interactions per day.\n",
        "\n",
        "## Machine Learning with Spark MLlib:\n",
        "- Apply the knowledge gained in Part 2 to build a machine learning model using Spark MLlib.\n",
        "- Perform the following steps:\n",
        "  1. **Data Preparation**: Convert your dataset into a format suitable for machine learning, using techniques such as feature indexing and vector assembly.\n",
        "  2. **Model Training**: Train a classification model (e.g., RandomForestClassifier) to predict customer churn.\n",
        "  3. **Model Evaluation**: Evaluate your model using metrics such as accuracy, precision, and recall.\n",
        "\n",
        "## Visualization and Reporting:\n",
        "- Create meaningful static and interactive visualizations to represent your findings using Spark or external libraries like `matplotlib` or `plotly`.\n",
        "- Craft visual representations of your analytical findings to showcase patterns and insights effectively.\n",
        "\n",
        "## Instructions:\n",
        "1. Load the 'Telecom Customer Churn Dataset' into Spark and perform initial explorations to understand its structure.\n",
        "2. Conduct necessary data cleaning and feature engineering to prepare your data for deeper analysis.\n",
        "3. Use Spark SQL to perform detailed analyses and create relevant aggregations.\n",
        "4. Build and evaluate a machine learning model using Spark MLlib.\n",
        "5. Compile your steps, insights, and model results into the Jupyter notebook and submit it as your completed assignment.\n"
      ],
      "metadata": {
        "id": "UaLj6t02fxe3"
      },
      "id": "UaLj6t02fxe3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: Replace 'telcom_customer_churn.csv' with the actual file path.\n",
        "telcom_customer_churn_df = pd.read_csv('telecom_customer_churn.csv')"
      ],
      "metadata": {
        "id": "S8-AHO16lzKa"
      },
      "id": "S8-AHO16lzKa",
      "execution_count": 8,
      "outputs": []
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}